>   **SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 12      |
|-----------------|
| Student 1: Andrew Lee                |   
| Student 2: Dillon Pullano             |   
| Student 3: Jonathan Lui               |   
| Student 4: Gi-E Thang               |   
| Student 5: Yaad Sra               | 


**Table of Contents**


[1 Introduction    1](#_Toc439194677)

[2 High-level description of the exploratory testing plan    1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing    1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports    1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided    1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned    1](#_Toc439194682)

# Introduction

Before this lab we had limited knowledge on exploratory testing. However, some of us did have previous knowledge about manual testing. This plan is created so that we can formulate a proposed exploratory testing strategy which can be followed to streamline bug testing for the ATM system within our group. The general approach to testing that we will be following is discussed first. Then we highlight which functions will be targeted for testing and which will be ignored. The criteria for what information should be included in each bug report are then presented.

# High-level description of the exploratory testing plan

The first step in our approach is to read through the high-level requirements (Appendix B) and make a list of requirements that can be tested for. For each requirement, a set of exploratory test cases can be performed individually so that each tester can achieve a better understanding of how the system functions overall. 

Our goal for the exploratory (manual non-scripted) testing stage is to test all targeted functions in a non-exhaustive approach. Half of us will run our tests with the first card (PIN = 42) while the other half will run exploratory tests with the second card (PIN = 1234) for ATM V1.0. Testers are encouraged to interpret what functionality associated with each testing target is most important to them. We hope that this approach results in tests that reflect how a normal user would commonly try to use the system.

Functions / Test Cases being targeted:
The following functionalities have been identified as targets for the exploratory bug testing of the ATM system. They were derived through reading the high-level system requirements found in Appendix B of the assignment handout:

Log in with PIN
, Log out (card returned)
, Test invalid inputs for options displayed
, Cash deposit to all accounts linked to the card
, Cash withdrawal from all accounts linked to card
, Transfers between accounts linked to the card
, Balance inquiry for all accounts linked to the card
, Abort transaction in progress through cancel button instead of responding to request from the machine
, Insert envelope timeout limit
, Invalid PIN checks and account / card lock after 3 wrong tries
, Failed transactions being logged correctly
, ‘Another Transaction’ button
, System initialization and available money for ATM
, System shutdown
, Log tracking all messages correctly (failures, dispensing cash, receiving cash)
, Log clear of sensitive PIN information (can include card numbers and $)
, Clearing log

Functions NOT being targeted:
Anything related to the communication between the ATM and the bank
, Removing and reloading of envelopes by operator after shutdown
, Any other physical aspect of the system

Bug Testing Report Criteria:
The following will be included in each bug report. Most of these were suggested by the assignment handout but we added some additional columns for organisational purposes:

Test Case ID
, Names of test case creator, reviewer, and tester (same in this case)
, Date test was completed
, Which card number (1 or 2) was used to complete the test
, Name of the function being tested
, Initial state of the system before the test is started
, Steps / actions required to reproduce the defect
, Expected outcome of the test
, Actual outcome of the test
, Priority and severity of each bug
, Version of the SUT in which the bug was found
, Status of the bug after the regression test
, Testing Environment (Windows or Mac)

# Comparison of exploratory and manual functional testing

Exploratory bug analysis and manual functional testing are two distinct approaches to software testing, each with its unique focus and methodology. Exploratory bug analysis is an unscripted, adaptive testing method where we actively explore and experiment with the software to identify bugs without following a predefined set of test cases. We found that this approach emphasised the tester's creativity, intuition, and experience, allowing for the discovery of issues that might not be found through structured testing. 

On the other hand, manual functional testing is a systematic approach where we execute a predefined set of test cases to verify that the software functions as expected. This method is more structured and aims to ensure that all specified requirements are met, covering specific features and functionalities. While exploratory testing is dynamic and helps uncover hard-to-find bugs, manual functional testing provides a comprehensive assessment of the software's intended functionality.

We found that there was overlap between exploratory and manual functional testing. This makes sense as there should be some overlap between what new users might want to explore themselves and what the client may have defined in their high level requirements. Manual testing is a more effective tool to meet the requirements provided by the client but it is more time consuming. Exploratory testing is more likely to capture requirements that might not be so obvious to the client but it is associated with a steeper learning curve.


# Notes and discussion of the peer reviews of defect reports

With the grouped pair testing (1 group of 2 and 1 group of 3), we had two defect reports before we met up as a group again. After completing the exploratory testing phase, we engaged in a collaborative review process. Each group thoroughly examined the defect reports written by the other, and every team member tested the defect reports from their counterpart group. This dual-review approach guaranteed a minimum of two peer reviews for each defect report, proving instrumental in detecting overlapping and false-positive bugs. 

The examination of defect reports became particularly valuable when detailing the steps to reproduce a bug and outlining the expected outcome, as it facilitated a more thorough understanding of the reported issues. Knowledge sharing between the two groups allowed for the fostering of insights into different approaches and problem-solving techniques between our group members. However, this approach was more time consuming because we had to make a defect report as well as review another group's report. 


# How the pair testing was managed and team work/effort was divided 

Following the structure outlined in the lab manual, our team divided into two groups to engage in pair-testing. Within each pair, one member assumed the responsibility of reading out tests and documenting identified bugs, while the other focused on scrutinising the ATM executable jar file to uncover potential issues. To ensure comprehensive testing, the groups were assigned different cards—card 1 for one group and card 2 for the other. In the end, we combined the two defect reports and made changes to the end result of the defect report. 

For manual testing, we all got into a call, and one person shared the ATM screen while the rest of the group read and wrote the testing of each test case. We then, as a group, added the manual testing to the defect report. For the regression testing, we all individually looked through each bug and checked the bugs in V1.1 to see whether the bug was fixed and if there were any new bugs. As for the lab report, we all chose a section to complete and then reviewed the whole document as a whole team before submission.


# Difficulties encountered, challenges overcome, and lessons learned

This lab has been helpful as an introduction to exploratory, manual, and regression software testing. We learned how to perform exploratory bug testing within an application starting with the high level system requirements provided by a client. We also learned how to set up projects and create tickets with Jira. We found it challenging to set the priority and severity of issues as we found it to be subjective to some extent. Some features are considered more important to some than others.

We found this assignment to be useful in teaching us the general processes involved in bug testing. Learning and practising the process of breaking down requirements into identified test cases, as well as performing exploratory, manual scripted, and regression testing was helpful for learning. We found that the exploratory section was a little unclear on how to set up the initial cases. Once we got into the manual scripted test section it started to make more sense to us.


